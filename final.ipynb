{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ulugsali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ulugsali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Dataset 1:\n",
      "                                     avatarThumbnail                  cid  \\\n",
      "0  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...  7247915369984312070   \n",
      "1  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...  7216521968188392238   \n",
      "2  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...  7247888310218408710   \n",
      "3  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...  7216606811569488682   \n",
      "4  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...  7216822967849566982   \n",
      "\n",
      "                 createTime             createTimeISO  diggCount  repliesToId  \\\n",
      "0 2023-06-23 16:13:32+00:00  2023-06-23T16:13:32.000Z   0.016535          NaN   \n",
      "1 2023-03-31 01:51:10+00:00  2023-03-31T01:51:10.000Z   0.020418          NaN   \n",
      "2 2023-06-23 14:28:30+00:00  2023-06-23T14:28:30.000Z   0.004627          NaN   \n",
      "3 2023-03-31 07:20:56+00:00  2023-03-31T07:20:56.000Z   0.011495          NaN   \n",
      "4 2023-03-31 21:19:16+00:00  2023-03-31T21:19:16.000Z   0.008653          NaN   \n",
      "\n",
      "   replyCommentTotal                                  submittedVideoUrl  \\\n",
      "0           0.025909  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "1           0.000836  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "2           0.005433  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "3           0.000418  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "4           0.010029  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "\n",
      "                                                text                  uid  \\\n",
      "0                         maybe simpson real cartoon  6855815825585947650   \n",
      "1                             think designer purpose  6759018222479295493   \n",
      "2           waiting model back flip like one simpson  7094989553283302405   \n",
      "3  collab balenciaga yall look thing simpson put ...  7177493171948733483   \n",
      "4                                      video created  7202522368282313729   \n",
      "\n",
      "         uniqueId                                        videoWebUrl  \\\n",
      "0       evannnn71  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "1      s.e.clarke  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "2         tee_057  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "3  lana.del..slay  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "4   ladyinbeige23  https://www.tiktok.com/@hoangphibaby/video/721...   \n",
      "\n",
      "   day_of_week  hour  \n",
      "0            4    16  \n",
      "1            4     1  \n",
      "2            4    14  \n",
      "3            4     7  \n",
      "4            4    21  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>diggCount</th>\n",
       "      <th>repliesToId</th>\n",
       "      <th>replyCommentTotal</th>\n",
       "      <th>uid</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.113600e+04</td>\n",
       "      <td>21136.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21136.000000</td>\n",
       "      <td>2.113600e+04</td>\n",
       "      <td>21136.000000</td>\n",
       "      <td>21136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.183996e+18</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004865</td>\n",
       "      <td>6.489387e+18</td>\n",
       "      <td>3.030564</td>\n",
       "      <td>12.168149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.631011e+17</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>1.607452e+18</td>\n",
       "      <td>1.969297</td>\n",
       "      <td>7.230820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.614742e+15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.720100e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.098999e+18</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.745077e+18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.212062e+18</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>6.837581e+18</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.306264e+18</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>7.003809e+18</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.363034e+18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.362077e+18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                cid     diggCount  repliesToId  replyCommentTotal  \\\n",
       "count  2.113600e+04  21136.000000          0.0       21136.000000   \n",
       "mean   7.183996e+18      0.011551          NaN           0.004865   \n",
       "std    1.631011e+17      0.040078          NaN           0.015513   \n",
       "min    1.614742e+15      0.000000          NaN           0.000000   \n",
       "25%    7.098999e+18      0.000011          NaN           0.000000   \n",
       "50%    7.212062e+18      0.000359          NaN           0.000418   \n",
       "75%    7.306264e+18      0.004803          NaN           0.003343   \n",
       "max    7.363034e+18      1.000000          NaN           1.000000   \n",
       "\n",
       "                uid   day_of_week          hour  \n",
       "count  2.113600e+04  21136.000000  21136.000000  \n",
       "mean   6.489387e+18      3.030564     12.168149  \n",
       "std    1.607452e+18      1.969297      7.230820  \n",
       "min    9.720100e+04      0.000000      0.000000  \n",
       "25%    6.745077e+18      1.000000      5.000000  \n",
       "50%    6.837581e+18      3.000000     13.000000  \n",
       "75%    7.003809e+18      5.000000     19.000000  \n",
       "max    7.362077e+18      6.000000     23.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 21136\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def load_and_filter_data(file_path, text_column):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file and filter rows where the text column contains text.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    text_column (str): Name of the column containing text comments.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame with non-empty text comments.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    filtered_data = data[data[text_column].notna() & (data[text_column] != '')]\n",
    "    return filtered_data\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame with text and numerical data.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess the 'text' column\n",
    "    df['text'] = df['text'].str.lower().str.replace(r'\\d+', '', regex=True).str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    # Normalize numerical columns with separate scalers\n",
    "    scaler_digg = MinMaxScaler()\n",
    "    scaler_reply = MinMaxScaler()\n",
    "    df['diggCount'] = scaler_digg.fit_transform(df[['diggCount']])\n",
    "    df['replyCommentTotal'] = df['replyCommentTotal'].fillna(0)\n",
    "    df['replyCommentTotal'] = scaler_reply.fit_transform(df[['replyCommentTotal']])\n",
    "\n",
    "    # Convert and extract datetime features\n",
    "    df['createTime'] = pd.to_datetime(df['createTimeISO'])\n",
    "    df['day_of_week'] = df['createTime'].dt.dayofweek\n",
    "    df['hour'] = df['createTime'].dt.hour\n",
    "\n",
    "    return df\n",
    "\n",
    "# Paths to your dataset files\n",
    "file_path1 = 'dataset_tiktok-comments-scraper_2024-04-28_23-16-10-409.csv'\n",
    "# file_path2 = 'dataset_free-tiktok-scraper_2024-04-28_21-22-00-488.csv'\n",
    "\n",
    "# Load and filter datasets\n",
    "dataset1 = load_and_filter_data(file_path1, 'text')  # Assuming 'text' is the column for Dataset 1\n",
    "# dataset2 = load_and_filter_data(file_path2, 'text')  # Update 'text' if a different column name for Dataset 2\n",
    "\n",
    "# Preprocess datasets\n",
    "dataset1 = preprocess_data(dataset1)\n",
    "# dataset2 = preprocess_data(dataset2)\n",
    "\n",
    "# Display the first few rows of the preprocessed datasets\n",
    "print(\"Preprocessed Dataset 1:\")\n",
    "print(dataset1.head())\n",
    "display(dataset1.describe())\n",
    "# print(\"\\nPreprocessed Dataset 2:\")\n",
    "# print(dataset2.head())\n",
    "\n",
    "rows, columns = dataset1.shape\n",
    "\n",
    "print(\"Number of rows:\", rows)\n",
    "print(\"Number of columns:\", columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c7eb99bb824cfdaf123ec91fd64b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e68c2384b34ea5a77632725f7da777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b932e290be07473d915ed8505c9009a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc43ac43e5349ab981c763389cb3a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b52c775abd4a1f8d70e80e992f2e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pretrained model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamLowe/roberta-base-go_emotions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamLowe/roberta-base-go_emotions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example of how to prepare text data for the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1450\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1450\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1438\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1436\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Example of how to prepare text data for the model\n",
    "text = \"This is a sample text.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BrandPerceptionDataset.__init__() missing 3 required positional arguments: 'aspect_labels', 'tokenizer', and 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m input_ids, attention_mask, aspect_label\n\u001b[0;32m---> 24\u001b[0m \u001b[43mBrandPerceptionDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: BrandPerceptionDataset.__init__() missing 3 required positional arguments: 'aspect_labels', 'tokenizer', and 'max_length'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BrandPerceptionDataset(Dataset):\n",
    "    def __init__(self, texts, aspect_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.aspect_labels = aspect_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        aspect_label = self.aspect_labels[idx]\n",
    "        \n",
    "        # Tokenize text and convert to input_ids and attention_mask\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return input_ids, attention_mask, aspect_label\n",
    "\n",
    "BrandPerceptionDataset(dataset1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
