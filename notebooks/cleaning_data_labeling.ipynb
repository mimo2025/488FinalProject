{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Gauging Brand Perception: How Luxury Brands Can Stay One Step Ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Please, read the README.md before starting to run the project\n",
    "- Team members: Carley Wiley, Eldar Utiushev, Bek Tukhtasinov, Mira Mohan, Tammy Duong, and Aryonna Rice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing and downloading the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the analysis, it's necessary to install specific Python libraries that our scripts depend on. Below are the commands used to install these libraries:\n",
    "\n",
    "- **langdetect**: This library is used to detect the language of a given text automatically. It supports detection of several languages and is very useful for preprocessing steps in NLP tasks where language-specific processing might be necessary.\n",
    "\n",
    "- **scikit-learn**: A powerful and versatile machine learning library in Python. It includes a wide range of tools for modeling and transforming data, including classification, regression, clustering, and dimensionality reduction. We use this library for various machine learning tasks throughout our analysis.\n",
    "\n",
    "- **openai**: Upgrades the openai library to the latest version. This library provides API access to OpenAI's GPT models, allowing for easy integration of AI-powered natural language processing into applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/ulugsali/Library/Python/3.10/lib/python/site-packages (from langdetect) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install scikit-learn\n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code snippet involves importing various libraries and setting up the environment for a Natural Language Processing (NLP) task that involves text processing, language detection, machine learning, and deep learning functionalities. Below is an explanation of each component and its purpose:\n",
    "\n",
    "### Library Imports:\n",
    "- **Pandas**: Used for data manipulation and analysis. It offers data structures and operations for manipulating numerical tables and time series.\n",
    "- **re**: Provides regular expression matching operations similar to those found in Perl, used for string searching and manipulation.\n",
    "- **nltk (Natural Language Toolkit)**: A suite of libraries and programs for symbolic and statistical natural language processing. It includes facilities for tokenizing, part-of-speech tagging, stemming, and more.\n",
    "- **langdetect**: A library for detecting the language of text.\n",
    "- **sklearn (Scikit-learn)**: A machine learning library for Python. It features various classification, regression, and clustering algorithms.\n",
    "- **openai**: A library to interface with OpenAI's GPT models and other AI tools provided by OpenAI.\n",
    "- **numpy**: Fundamental package for scientific computing with Python, providing support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "- **torch (PyTorch)**: An open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing.\n",
    "- **transformers**: Provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages.\n",
    "- **datasets**: A library for easily accessing and sharing datasets for machine learning tasks.\n",
    "\n",
    "### Resource Downloads:\n",
    "- **nltk resources**: Downloading necessary datasets for tokenization (`punkt`), stopwords (`stopwords`), and lemmatization (`wordnet`) which are essential for text preprocessing.\n",
    "\n",
    "### Environment Setup:\n",
    "- The code ensures all necessary libraries and packages are imported for tasks ranging from data manipulation, model training, and evaluation to working with state-of-the-art NLP models. This setup is crucial for handling complex workflows in data science projects that involve text analysis and machine learning.\n",
    "\n",
    "### Use Cases:\n",
    "- **Train/Test Split**: The `train_test_split` function from `sklearn` is used to divide the dataset into training and testing sets, which is a common practice in machine learning to evaluate model performance.\n",
    "- **Model Training and Evaluation**: Using libraries like `torch` and `transformers` for training deep learning models and evaluating them using metrics like precision, recall, and F1-score.\n",
    "- **Krippendorff's alpha**: A statistical measure of the agreement level among multiple raters for qualitative (categorical) items, indicating the reliability of the raters.\n",
    "\n",
    "This code is typically used in a setup phase of a project where machine learning or deep learning models are trained for tasks such as sentiment analysis, language detection, or any other form of textual data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ulugsali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ulugsali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ulugsali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAIError\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import krippendorff\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, IntervalStrategy\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datetime import datetime\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the scraped data from TikTok related to fashion brands described in the video submission and presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maybe the simpsons are real and we are the car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think the designers do this on purpose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not me waiting for the model to do the back fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this was a collab they did with balenciaga yal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the videos were created after 😂</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Maybe the simpsons are real and we are the car...\n",
       "1           I think the designers do this on purpose\n",
       "2  not me waiting for the model to do the back fl...\n",
       "3  this was a collab they did with balenciaga yal...\n",
       "4                    the videos were created after 😂"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data\n",
    "df = pd.read_csv('tiktok_comments.csv')\n",
    "\n",
    "# we only need a text column in tiktok_comments dataset since it's the only\n",
    "# thing in this dataset that can help us gauge the brand perception. \n",
    "df = df[['text']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the loaded text data for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing Workflow\n",
    "\n",
    "This section of the code is dedicated to preparing text data for natural language processing (NLP) by performing several preprocessing steps. These steps are essential for cleaning and standardizing the text data to improve the performance of NLP models. Below is a detailed explanation of each function and its role in the preprocessing pipeline:\n",
    "\n",
    "Functions Defined:\n",
    "\n",
    "- **`clean_text`**:\n",
    "  - **Purpose**: Cleans the text by removing URLs, special characters, punctuation, and converting all text to lowercase.\n",
    "  - **Implementation**:\n",
    "    - URLs are removed using a regular expression that identifies strings that start with `http` or `www`.\n",
    "    - Non-alphanumeric characters and punctuation are removed with a regex that matches any character that is not a word character or whitespace.\n",
    "    - Converts all characters in the text to lowercase to standardize the case.\n",
    "\n",
    "- **`remove_stopwords`**:\n",
    "  - **Purpose**: Filters out stopwords from the text, which are commonly used words (such as \"the\", \"a\", \"an\", \"in\") that may not be useful for some NLP tasks.\n",
    "  - **Implementation**:\n",
    "    - Utilizes the `stopwords` list from NLTK library, which is a well-curated list of stopwords for the English language.\n",
    "    - Tokenizes the text into individual words and filters out any words that are in the stopwords list.\n",
    "    - Rejoins the filtered words into a single string.\n",
    "\n",
    "- **`detect_language`**:\n",
    "  - **Purpose**: Detects the language of the text using the `langdetect` library, which can recognize multiple languages based on the textual input.\n",
    "  - **Implementation**:\n",
    "    - Attempts to detect the language and returns the language code (e.g., \"en\" for English).\n",
    "    - If detection fails (possibly due to insufficient text), it returns 'unknown'.\n",
    "\n",
    "### Data Cleaning Process:\n",
    "\n",
    "- **Applying Functions**:\n",
    "  - The `clean_text` function is applied to the raw text data in the DataFrame to perform initial cleaning.\n",
    "  - The `remove_stopwords` function is then applied to the cleaned text to filter out stopwords.\n",
    "  - The `detect_language` function is applied last to determine the language of the filtered text.\n",
    "\n",
    "- **Filtering Non-English Comments**:\n",
    "  - Only rows where the detected language is English ('en') are retained. This is crucial for tasks that are specifically designed for English language data.\n",
    "\n",
    "- **Handling Missing Values**:\n",
    "  - Drops any rows where the 'filtered_text' column is empty after preprocessing, ensuring that the dataset does not contain any null or empty values that could interfere with further analysis.\n",
    "\n",
    "### Usage:\n",
    "\n",
    "This preprocessing pipeline is typically used in the initial stages of a text analysis project to ensure that the text data is clean and uniform, making it more amenable to analysis and modeling. It lays the foundation for reliable and accurate insights from subsequent NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Preprocess and clean data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m     24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_stopwords)\n\u001b[1;32m     25\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(detect_language)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# Preprocess and clean data\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "df['filtered_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "df['language'] = df['filtered_text'].apply(detect_language)\n",
    "df = df[df['language'] == 'en']  # Keep only English language comments\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(subset=['filtered_text'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['filtered_text'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiltered_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Optionally, if you want to rename the column to something more general like 'text', you can do:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered_text\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3810\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3809\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3810\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:6111\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6109\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6111\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6113\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6115\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:6171\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6170\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6173\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6174\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['filtered_text'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df = df[['filtered_text']]\n",
    "\n",
    "# Optionally, if you want to rename the column to something more general like 'text', you can do:\n",
    "df.rename(columns={'filtered_text': 'text'}, inplace=True)\n",
    "\n",
    "# Now 'df' contains only the column with the cleaned and filtered text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('filtered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Splitting into Train, Test and Validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 6772\n",
      "Validation set size: 500\n",
      "Testing set size: 1452\n"
     ]
    }
   ],
   "source": [
    "# First, separate out a random 500 rows for validation\n",
    "validation_df = df.sample(n=500, random_state=42)\n",
    "remaining_df = df.drop(validation_df.index)  # Drop the validation rows from the original dataset\n",
    "\n",
    "# Now, split the remaining data into training and testing sets\n",
    "# We need to adjust the proportions since 500 rows are already taken by the validation set\n",
    "remaining_train_percentage = 0.7 / (0.85)  # Adjusting for 70% of the original now being a different percentage of the remaining\n",
    "train_df, test_df = train_test_split(remaining_df, test_size=(1 - remaining_train_percentage), random_state=42)\n",
    "\n",
    "# Print the sizes of each dataset to verify\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(validation_df)}\")  # Should be exactly 500\n",
    "print(f\"Testing set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spliting data into training, validation and testing sets\n",
    "filtered_df = pd.read_csv('filtered_data.csv')\n",
    "\n",
    "# First, separate out a random 500 rows for validation\n",
    "validation_df = filtered_df.iloc[-500::,:]\n",
    "\n",
    "# Now, split the remaining data into training and testing sets\n",
    "train_test_df = filtered_df.iloc[0:-500,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maybe simpsons real cartoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think designers purpose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>waiting model back flip like ones simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>collab balenciaga yall looks thing simpson put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>videos created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>wheres hood scarf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>cus dubai thats nissan altima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>park toyota dubai switch mercedes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8722</th>\n",
       "      <td>south africa everything would gone including p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>thanks god thats love homeالحمدالله</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8724 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                           maybe simpsons real cartoon\n",
       "1                               think designers purpose\n",
       "2            waiting model back flip like ones simpsons\n",
       "3     collab balenciaga yall looks thing simpson put...\n",
       "4                                        videos created\n",
       "...                                                 ...\n",
       "8719                                  wheres hood scarf\n",
       "8720                      cus dubai thats nissan altima\n",
       "8721                  park toyota dubai switch mercedes\n",
       "8722  south africa everything would gone including p...\n",
       "8723                thanks god thats love homeالحمدالله\n",
       "\n",
       "[8724 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8224</th>\n",
       "      <td>opium founding father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8225</th>\n",
       "      <td>yall trippin fit clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>might destroy lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>alr show us women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8228</th>\n",
       "      <td>bad think jeans ripped pull bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>wheres hood scarf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>cus dubai thats nissan altima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>park toyota dubai switch mercedes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8722</th>\n",
       "      <td>south africa everything would gone including p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>thanks god thats love homeالحمدالله</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "8224                              opium founding father\n",
       "8225                             yall trippin fit clean\n",
       "8226                               might destroy lonely\n",
       "8227                                  alr show us women\n",
       "8228                    bad think jeans ripped pull bad\n",
       "...                                                 ...\n",
       "8719                                  wheres hood scarf\n",
       "8720                      cus dubai thats nissan altima\n",
       "8721                  park toyota dubai switch mercedes\n",
       "8722  south africa everything would gone including p...\n",
       "8723                thanks god thats love homeالحمدالله\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maybe simpsons real cartoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think designers purpose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>waiting model back flip like ones simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>collab balenciaga yall looks thing simpson put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>videos created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219</th>\n",
       "      <td>true philippines fashion shorts tsinelas sando...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8220</th>\n",
       "      <td>philippines fashion thats underrated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8221</th>\n",
       "      <td>average fashion art music major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>wheres bag omg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8223</th>\n",
       "      <td>blud think willy wonka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8224 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                           maybe simpsons real cartoon\n",
       "1                               think designers purpose\n",
       "2            waiting model back flip like ones simpsons\n",
       "3     collab balenciaga yall looks thing simpson put...\n",
       "4                                        videos created\n",
       "...                                                 ...\n",
       "8219  true philippines fashion shorts tsinelas sando...\n",
       "8220               philippines fashion thats underrated\n",
       "8221                    average fashion art music major\n",
       "8222                                     wheres bag omg\n",
       "8223                             blud think willy wonka\n",
       "\n",
       "[8224 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets to CSV files:\n",
    "train_test_df.to_csv('train_test_data.csv', index=False)\n",
    "validation_df.to_csv('validation_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df.to_json('train_test_data.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maybe simpsons real cartoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think designers purpose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>waiting model back flip like ones simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>collab balenciaga yall looks thing simpson put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>videos created</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                        maybe simpsons real cartoon\n",
       "1                            think designers purpose\n",
       "2         waiting model back flip like ones simpsons\n",
       "3  collab balenciaga yall looks thing simpson put...\n",
       "4                                     videos created"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test = pd.read_csv('train_test_data.csv')\n",
    "len(train_test)\n",
    "train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Coverting to an array of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       8224\n",
       "unique      8014\n",
       "top       first,\n",
       "freq          17\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to convert each sentence to the desired format\n",
    "def convert_to_array(text):\n",
    "    return f'{text},'\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "comments = train_test['text'].apply(convert_to_array)\n",
    "comments.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. OpenAI API Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and API Configuration\n",
    "client = OpenAI(api_key = \"enter_your_api\")\n",
    "# We removed our API key for security reasons!\n",
    "\n",
    "# 2. Define a function to query OpenAI's models via the API\n",
    "def ask_gpt(System_Prompt, User_Query, tokens, temp=1.0, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, model=\"gpt-4-turbo\"):\n",
    "    \"\"\"Function that Queries OpenAI API\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": System_Prompt},\n",
    "            {\"role\": \"user\", \"content\": User_Query}],\n",
    "        max_tokens=tokens,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            maybe simpsons real cartoon,\n",
       "1                                think designers purpose,\n",
       "2             waiting model back flip like ones simpsons,\n",
       "3       collab balenciaga yall looks thing simpson put...\n",
       "4                                         videos created,\n",
       "                              ...                        \n",
       "8219    true philippines fashion shorts tsinelas sando...\n",
       "8220                philippines fashion thats underrated,\n",
       "8221                     average fashion art music major,\n",
       "8222                                      wheres bag omg,\n",
       "8223                              blud think willy wonka,\n",
       "Name: text, Length: 8224, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Define your Prompt:\n",
    "\n",
    "## System: Role, task, and format\n",
    "System_Prompt = \"\"\"\n",
    "You are an AI expert trained to analyze social media comments for insights into fashion brand perceptions and emotional sentiments. Your task is to interpret each comment, even when the references to brands or sentiments are indirect, and classify the comment into applicable categories based on the information provided.\n",
    "\n",
    "Brand perception categories:\n",
    "1. Product Quality: Indicate if the comment suggests any perception about the quality of fashion products.\n",
    "2. Reputation & Heritage: Determine if the comment reflects on the brand's history or reputation in the fashion industry.\n",
    "3. Customer Service: Assess if there are any mentions or implications regarding customer service experiences with fashion brands.\n",
    "4. Social Impact: Consider if the comment discusses the brand's role or actions in social issues.\n",
    "5. Ethical Practices: Evaluate any mentions of ethical practices by the fashion brand.\n",
    "6. Sustainability: Look for any discussions related to environmental sustainability of the brand.\n",
    "\n",
    "Emotional sentiments to identify based on the presence of specific keywords:\n",
    "- 'love', 'great', 'amazing', 'impressed', 'thrilled', 'excited'\n",
    "- 'horrible', 'bad', 'disappointed', 'terrible', 'worse', 'hate'\n",
    "\n",
    "Analyze the text, assign it to relevant brand perception categories, and determine the emotional sentiment based on the keywords or overall tone of the comment. If the comment does not contain explicit keywords, use your judgment to infer the sentiment from the context. Provide your findings in a structured format, listing both the identified brand perception categories and the emotional sentiment.\n",
    "If the comment even slightly vaguely implies something categorize it into categories i provided. If it's a reference or a very short phrase, still categoprize it. Don't leave any comment unlabeled in terms of brand perceotion and emotional sentiment.\n",
    "\n",
    "For each comment, determine which brand perception categories and emotional sentiments apply. Provide your analysis in a structured format, with two columns: one for 'brand_labels' and one for 'emotion_labels'. List all applicable categories and sentiments in their respective columns, formatted as arrays. For example, a comment that is classified under both 'product quality' and 'reputation & heritage' for brand perception, and 'love' and 'admiration' for emotional sentiment, should be formatted as follows:\n",
    "\n",
    "Brand_labels: [\"product quality\", \"reputation & heritage\"]\n",
    "Emotion_labels: [\"love\", \"admiration\"]\n",
    "\n",
    "Please ensure to apply this structured approach to all comments, making judgment calls as necessary based on even slight or indirect implications within the comments.\n",
    "\"\"\"\n",
    "\n",
    "User_Query = comments[0]\n",
    "display(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9KvM6med0bfBDNGBAYL9T8gXLmRqb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Brand_labels: [\"Reputation & Heritage\"]\\nEmotion_labels: [\"Neutral\"]', role='assistant', function_call=None, tool_calls=None))], created=1714774062, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=540, total_tokens=556))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand_labels: [\"Reputation & Heritage\"]\n",
      "Emotion_labels: [\"Neutral\"]\n"
     ]
    }
   ],
   "source": [
    "# 3. Use API to connect with AI model and get model response\n",
    "response = ask_gpt(System_Prompt, User_Query, tokens=1000, temp=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 4. Show response\n",
    "display(response)\n",
    "\n",
    "# 5. Get Response Content\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define an OpenAI function\n",
    "myfunction = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"predict_label\",\n",
    "    \"description\": \"Identify the brand perception category/categories and emotional sentiments based on the text\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"brand_labels\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\n",
    "              \"product quality\", \n",
    "              \"reputation & heritage\", \n",
    "              \"customer service\", \n",
    "              \"social impact\", \n",
    "              \"ethical practices\",\n",
    "              \"sustainability\"\n",
    "            ]\n",
    "          },\n",
    "          \"description\": \"Brand perception labels for the social media comments.\"\n",
    "        },\n",
    "        \"emotion_labels\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\n",
    "              \"love\", \n",
    "              \"admiration\", \n",
    "              \"disgust\", \n",
    "              \"disapproval\", \n",
    "              \"excitement\", \n",
    "              \"optimism\", \n",
    "              \"disappointment\", \n",
    "              \"approval\", \n",
    "              \"pride\"\n",
    "            ]\n",
    "          },\n",
    "          \"description\": \"Emotional sentiment labels for the social media comments.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"brand_labels\", \n",
    "        \"emotion_labels\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define a Python Function to query an OpenAI Model\n",
    "def classify_gpt(text, model=\"gpt-3.5-turbo\", tokens=200, temp=0.0, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"Function that uses OpenAI's API to classify text based on brand perception and emotional sentiment\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": System_Prompt},  # System prompt that explains the task to the model\n",
    "                {\"role\": \"user\", \"content\": text}  # User input that needs to be classified\n",
    "            ],\n",
    "            max_tokens=tokens,\n",
    "            temperature=temp,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty\n",
    "        )\n",
    "        # Correct way to access the response content and tokens\n",
    "        content = response.choices[0].message.content  # Directly accessing the 'content' attribute of the 'message' object\n",
    "        total_tokens = response.usage.total_tokens  # Directly accessing 'total_tokens' attribute of the 'usage' object\n",
    "        return content, total_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 comments\n",
      "Processed 100 comments\n",
      "Processed 150 comments\n",
      "Processed 200 comments\n",
      "Processed 250 comments\n",
      "Processed 300 comments\n",
      "Processed 350 comments\n",
      "Processed 400 comments\n",
      "Processed 450 comments\n",
      "Processed 500 comments\n",
      "Processed 550 comments\n",
      "Processed 600 comments\n",
      "Processed 650 comments\n",
      "Processed 700 comments\n",
      "Processed 750 comments\n",
      "Processed 800 comments\n",
      "Processed 850 comments\n",
      "Processed 900 comments\n",
      "Processed 950 comments\n",
      "Processed 1000 comments\n",
      "Processed 1050 comments\n",
      "Processed 1100 comments\n",
      "Processed 1150 comments\n",
      "Processed 1200 comments\n",
      "Processed 1250 comments\n",
      "Processed 1300 comments\n",
      "Processed 1350 comments\n",
      "Processed 1400 comments\n",
      "Processed 1450 comments\n",
      "Processed 1500 comments\n",
      "Processed 1550 comments\n",
      "Processed 1600 comments\n",
      "Processed 1650 comments\n",
      "Processed 1700 comments\n",
      "Processed 1750 comments\n",
      "Processed 1800 comments\n",
      "Processed 1850 comments\n",
      "Processed 1900 comments\n",
      "Processed 1950 comments\n",
      "Processed 2000 comments\n",
      "Processed 2050 comments\n",
      "Processed 2100 comments\n",
      "Processed 2150 comments\n",
      "Processed 2200 comments\n",
      "Processed 2250 comments\n",
      "Processed 2300 comments\n",
      "Processed 2350 comments\n",
      "Processed 2400 comments\n",
      "Processed 2450 comments\n",
      "Processed 2500 comments\n",
      "Processed 2550 comments\n",
      "Processed 2600 comments\n",
      "Processed 2650 comments\n",
      "Processed 2700 comments\n",
      "Processed 2750 comments\n",
      "Processed 2800 comments\n",
      "Processed 2850 comments\n",
      "Processed 2900 comments\n",
      "Processed 2950 comments\n",
      "Processed 3000 comments\n",
      "Processed 3050 comments\n",
      "Processed 3100 comments\n",
      "Processed 3150 comments\n",
      "Processed 3200 comments\n",
      "Processed 3250 comments\n",
      "Processed 3300 comments\n",
      "Processed 3350 comments\n",
      "Processed 3400 comments\n",
      "Processed 3450 comments\n",
      "Processed 3500 comments\n",
      "Processed 3550 comments\n",
      "Processed 3600 comments\n",
      "Processed 3650 comments\n",
      "Processed 3700 comments\n",
      "Processed 3750 comments\n",
      "Processed 3800 comments\n",
      "Processed 3850 comments\n",
      "Processed 3900 comments\n",
      "Processed 3950 comments\n",
      "Processed 4000 comments\n",
      "Processed 4050 comments\n",
      "Processed 4100 comments\n",
      "Processed 4150 comments\n",
      "Processed 4200 comments\n",
      "Processed 4250 comments\n",
      "Processed 4300 comments\n",
      "Processed 4350 comments\n",
      "Processed 4400 comments\n",
      "Processed 4450 comments\n",
      "Processed 4500 comments\n",
      "Processed 4550 comments\n",
      "Processed 4600 comments\n",
      "Processed 4650 comments\n",
      "Processed 4700 comments\n",
      "Processed 4750 comments\n",
      "Processed 4800 comments\n",
      "Processed 4850 comments\n",
      "Processed 4900 comments\n",
      "Processed 4950 comments\n",
      "Processed 5000 comments\n",
      "Processed 5050 comments\n",
      "Processed 5100 comments\n",
      "Processed 5150 comments\n",
      "Processed 5200 comments\n",
      "Processed 5250 comments\n",
      "Processed 5300 comments\n",
      "Processed 5350 comments\n",
      "Processed 5400 comments\n",
      "Processed 5450 comments\n",
      "Processed 5500 comments\n",
      "Processed 5550 comments\n",
      "Processed 5600 comments\n",
      "Processed 5650 comments\n",
      "Processed 5700 comments\n",
      "Processed 5750 comments\n",
      "Processed 5800 comments\n",
      "Processed 5850 comments\n",
      "Processed 5900 comments\n",
      "Processed 5950 comments\n",
      "Processed 6000 comments\n",
      "Processed 6050 comments\n",
      "Processed 6100 comments\n",
      "Processed 6150 comments\n",
      "Processed 6200 comments\n",
      "Processed 6250 comments\n",
      "Processed 6300 comments\n",
      "Processed 6350 comments\n",
      "Processed 6400 comments\n",
      "Processed 6450 comments\n",
      "Processed 6500 comments\n",
      "Processed 6550 comments\n",
      "Processed 6600 comments\n",
      "Processed 6650 comments\n",
      "Processed 6700 comments\n",
      "Processed 6750 comments\n",
      "Processed 6800 comments\n",
      "Processed 6850 comments\n",
      "Processed 6900 comments\n",
      "Processed 6950 comments\n",
      "Processed 7000 comments\n",
      "Processed 7050 comments\n",
      "Processed 7100 comments\n",
      "Processed 7150 comments\n",
      "Processed 7200 comments\n",
      "Processed 7250 comments\n",
      "Processed 7300 comments\n",
      "Processed 7350 comments\n",
      "Processed 7400 comments\n",
      "Processed 7450 comments\n",
      "Processed 7500 comments\n",
      "Processed 7550 comments\n",
      "Processed 7600 comments\n",
      "Processed 7650 comments\n",
      "Processed 7700 comments\n",
      "Processed 7750 comments\n",
      "Processed 7800 comments\n",
      "Processed 7850 comments\n",
      "Processed 7900 comments\n",
      "Processed 7950 comments\n",
      "Processed 8000 comments\n",
      "Processed 8050 comments\n",
      "Processed 8100 comments\n",
      "Processed 8150 comments\n",
      "Processed 8200 comments\n",
      "Total time: 172.15 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 8. Get all texts classified\n",
    "results = []\n",
    "tokens_used = 0\n",
    "review_count = 0\n",
    "\n",
    "# Query for each text the OpenAI model\n",
    "for text in comments:\n",
    "\n",
    "  # Error handling becomes important when you work with APIs. We imported OpenAIError to show errors and allow us to handle them\n",
    "  try:\n",
    "    response, tokens = classify_gpt(text, top_p=0.1, model=\"gpt-4-turbo\")\n",
    "    results.append((text, response, tokens))\n",
    "    tokens_used += tokens\n",
    "    review_count += 1\n",
    "    if review_count % 50 == 0 and review_count != 0:\n",
    "      print(f\"Processed {review_count} comments\") #Check to see the progress every 50 reviews\n",
    "  except OpenAIError as e:\n",
    "    # Handle all OpenAI API errors\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "if elapsed_time > 60:\n",
    "    print(f\"Total time: {elapsed_time / 60:.2f} minutes\")\n",
    "else:\n",
    "    print(f\"Total time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('maybe simpsons real cartoon,', 'Brand_labels: [\"product quality\", \"reputation & heritage\", \"customer service\", \"social impact\", \"ethical practices\", \"sustainability\"]\\nEmotion_labels: [\"love\", \"great\", \"amazing\", \"impressed\", \"thrilled\", \"excited\", \"horrible\", \"bad\", \"disappointed\", \"terrible\", \"worse\", \"hate\"]', 613)\n",
      "0       Brand_labels: [\"product quality\", \"reputation ...\n",
      "1       Brand_labels: [\"reputation & heritage\"]\\nEmoti...\n",
      "2       Brand_labels: [\"reputation & heritage\"]\\nEmoti...\n",
      "3       Brand_labels: [\"product quality\"]\\nEmotion_lab...\n",
      "4       Brand_labels: [\"reputation & heritage\"]\\nEmoti...\n",
      "                              ...                        \n",
      "8219    Brand_labels: [\"product quality\"]\\nEmotion_lab...\n",
      "8220    Brand_labels: [\"reputation & heritage\"]\\nEmoti...\n",
      "8221    Brand_labels: [\"reputation & heritage\"]\\nEmoti...\n",
      "8222    Brand_labels: [\"customer service\"]\\nEmotion_la...\n",
      "8223    Brand_labels: [\"reputation & heritage\"]\\nEmoti...\n",
      "Name: labels, Length: 8224, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results[0])\n",
    "\n",
    "# 9. Create a dataframe from the results\n",
    "df = pd.DataFrame(results, columns=['Text', 'labels','Tokens_used'])\n",
    "df.head()\n",
    "print(df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. saving labeled dataset\n",
    "df.to_csv('labeled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cleaning the labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(label_string):\n",
    "    brand_labels = []\n",
    "    emotion_labels = []\n",
    "    \n",
    "    # Extract brand labels\n",
    "    brand_start = label_string.find(\"Brand_labels: [\") + len(\"Brand_labels: [\")\n",
    "    brand_end = label_string.find(\"]\", brand_start)\n",
    "    brand_labels_str = label_string[brand_start:brand_end]\n",
    "    if brand_labels_str:\n",
    "        brand_labels = [label.strip().strip('\"') for label in brand_labels_str.split(\",\")]\n",
    "    \n",
    "    # Extract emotion labels\n",
    "    emotion_start = label_string.find(\"Emotion_labels: [\") + len(\"Emotion_labels: [\")\n",
    "    emotion_end = label_string.find(\"]\", emotion_start)\n",
    "    emotion_labels_str = label_string[emotion_start:emotion_end]\n",
    "    if emotion_labels_str:\n",
    "        emotion_labels = [label.strip().strip('\"') for label in emotion_labels_str.split(\",\")]\n",
    "    \n",
    "    return brand_labels, emotion_labels\n",
    "\n",
    "# Apply the parsing function to the labels column\n",
    "df[['brand_label', 'emotion_label']] = df['labels'].apply(lambda x: pd.Series(parse_labels(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['labels', 'Tokens_used'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>brand_label</th>\n",
       "      <th>emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8224</td>\n",
       "      <td>8224</td>\n",
       "      <td>8224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8014</td>\n",
       "      <td>34</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>first,</td>\n",
       "      <td>[reputation &amp; heritage]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17</td>\n",
       "      <td>3291</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Text              brand_label emotion_label\n",
       "count     8224                     8224          8224\n",
       "unique    8014                       34           111\n",
       "top     first,  [reputation & heritage]            []\n",
       "freq        17                     3291          2455"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing comments with empty emption label lists because they likely don't represent any kind of brand perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>brand_label</th>\n",
       "      <th>emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5769</td>\n",
       "      <td>5769</td>\n",
       "      <td>5769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5641</td>\n",
       "      <td>32</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>first,</td>\n",
       "      <td>[reputation &amp; heritage]</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17</td>\n",
       "      <td>2056</td>\n",
       "      <td>2414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Text              brand_label emotion_label\n",
       "count     5769                     5769          5769\n",
       "unique    5641                       32           110\n",
       "top     first,  [reputation & heritage]        [love]\n",
       "freq        17                     2056          2414"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows with empty emotion labels\n",
    "df = df[df['emotion_label'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Save the filtered data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving cleaned labeled data\n",
    "df.to_csv('labeled_data_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
